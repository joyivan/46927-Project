{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _This notebook, replicating the Figure 1 in the original paper, explores the function of each component in the closed-form influence function of logistic regression model._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data: Using the MINST data with 7 and 1 only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('MNIST_17.csv')\n",
    "X=data.iloc[:,1:]\n",
    "y=np.where(data['label']==7,1,-1)\n",
    "\n",
    "lmfit = LogisticRegression(C=0.00000001).fit(X.loc[2:], y[2:])\n",
    "theta = lmfit.coef_.T\n",
    "X=np.array(X.T)\n",
    "X_test,y_test = X[:,1],y[1]\n",
    "X,y = X[:,2:],y[2:]\n",
    "sigmoid = lambda x : 1/(1 + np.exp(-x))\n",
    "\n",
    "# Hessian\n",
    "H=np.zeros([X.shape[0],X.shape[0]])\n",
    "for i in range(len(X.T)):\n",
    "    H+=sigmoid(theta.T.dot(X[:,i]))*sigmoid(-theta.T.dot(X[:,i]))*np.outer(X[:,i],X[:,i])\n",
    "H=H*(1/X.shape[1])\n",
    "H_new=H+np.eye(H.shape[0])*0.001\n",
    "\n",
    "# each loss\n",
    "I_uploss=-y_test*y*sigmoid(-y_test*theta.T.dot(X_test))*sigmoid(-y*theta.T.dot(X))*X_test.T.dot(np.linalg.inv(H_new)).dot(X)\n",
    "I_uploss_noH=-y_test*y*sigmoid(-y_test*theta.T.dot(X_test))*sigmoid(-y*theta.T.dot(X))*X_test.T.dot(X)\n",
    "I_uploss_noT=-y_test*y*sigmoid(-y_test*theta.T.dot(X_test))*X_test.T.dot(np.linalg.inv(H_new)).dot(X)\n",
    "I_uploss_noTH=-y_test*y*sigmoid(-y_test*theta.T.dot(X_test))*X_test.T.dot(X)\n",
    "\n",
    "# put them together into a data frame\n",
    "d=pd.DataFrame(np.column_stack((I_uploss.T,I_uploss_noH.T,I_uploss_noT,I_uploss_noTH,y)))\n",
    "\n",
    "# calculate the index of the most harmful training plot with the same label as test \n",
    "idx=np.where(I_uploss==I_uploss.T[y==1].min())[1][0] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams['figure.figsize'] = (25,5)\n",
    "plt.subplot(1,5,1)\n",
    "plt.scatter(-d[2],-d[0],c=d[4])\n",
    "plt.xlabel('-I_up,loss (without train loss)')\n",
    "plt.ylabel('-I_up,loss')\n",
    "plt.subplot(1,5,2)\n",
    "plt.scatter(-d[1],-d[0],c=d[4])\n",
    "plt.xlabel('-I_up,loss (without H)')\n",
    "plt.ylabel('-I_up,loss')\n",
    "plt.subplot(1,5,3)\n",
    "plt.scatter(-d[3],-d[0],c=d[4])\n",
    "plt.xlabel('-I_up,loss (without train loss & H)')\n",
    "plt.ylabel('-I_up,loss')\n",
    "plt.subplot(1,5,4)\n",
    "plt.imshow(X_test.reshape(28,28))\n",
    "plt.title('Test Image')\n",
    "plt.subplot(1,5,5)\n",
    "plt.imshow(X[:,idx].reshape(28,28))\n",
    "plt.title('Harmful Training Image')\n",
    "\n",
    "#plt.savefig('[Fig]component influence.png', dpi=1000, bbox_inches='tight')\n",
    "plt.savefig('[Fig]component influence.eps', format='eps',dpi=700, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Comments:__\n",
    "\n",
    "This graph fits the pattern in Figure 1. Yellow dots are training data with the same label as test data; purple dots are the oppisite.\n",
    "\n",
    "__Left:__ Without the train loss term, we overestimate the influence of many training points: the points near the y=0 line should have I_up,loss close to 0, but instead have high influence when we remove the train loss term.\n",
    "__Mid:__ Without Hessian, all purple points are benefitial to the prediction (removing each point will increase the loss) while all yellow points are harmful (removing each point will decrease the loss). But this might not be true since some incorrect training data can have positive impact on the model as well. Also, this loss has a huge multiplier.\n",
    "__Right:__ Without training loss or Hessian, what is left is the scaled Euclidean inner product. This is similar to the mid graph but spread more widely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
