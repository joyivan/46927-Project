{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zed/anaconda/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "\n",
    "from data import Datafile, load_data\n",
    "from influence.emp_risk_optimizer import EmpiricalRiskOptimizer\n",
    "from influence.plot_utils import compare_with_loo\n",
    "from influence.closed_forms import I_loss_RidgeCf\n",
    "from models.regularized_regression import RegularizedRegression\n",
    "from models.hyperplane_clf import BinaryLogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (9084, 784)\n",
      "y_train shape: (9084, 1)\n",
      "X_test shape: (1, 784)\n",
      "y_test shape: (1, 1)\n",
      "9084 784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zed/anaconda/lib/python3.6/site-packages/sklearn/utils/validation.py:429: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "X_train, X_test, y_train, y_test, test_indices = load_data(\n",
    "    Datafile.BinaryMNIST17, test_config=[0])\n",
    "n_tr, p = X_train.shape\n",
    "n_te, _ = X_test.shape\n",
    "print(n_tr, p)\n",
    "\n",
    "\n",
    "scl_x = StandardScaler()\n",
    "scl_y = StandardScaler()\n",
    "X_train = scl_x.fit_transform(X_train)\n",
    "#X_test = scl_x.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "init_eta = 1e-1\n",
    "batch_size = 30\n",
    "train_iter = 50000\n",
    "traceback_checkpoint = 45000\n",
    "loo_extra_iter = 5000\n",
    "decay_epochs = (10000, 20000)\n",
    "checkpoint_iter = traceback_checkpoint-1\n",
    "iter_to_switch_off_minibatch=35000\n",
    "iter_to_switch_to_sgd=np.inf\n",
    "# LOO a on random set of training indices, otherwise too slow\n",
    "leave_indices = None\n",
    "\n",
    "if hasattr(test_indices, '__iter__') and hasattr(leave_indices, '__iter__'):\n",
    "    assert not set(test_indices) & set(leave_indices)\n",
    "    print(test_indices)\n",
    "    print(leave_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flip 10% indices\n",
    "np.random.seed(43)\n",
    "flip_indices = np.random.choice(n_tr, size=909, replace=False)\n",
    "y_train_fliped = np.copy(y_train)\n",
    "y_train_fliped[flip_indices,:] = 1-y_train_fliped[flip_indices,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BinaryLogisticRegression(\n",
    "    model_name='BinaryLogistic-MNIST',\n",
    "    init_eta=init_eta,\n",
    "    decay_epochs=decay_epochs,\n",
    "    batch_size=batch_size,\n",
    "    C=1e3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Evaluations:\n",
      "------------------------------------------\n",
      "Empirical Risk: 0.360206\n",
      "Norm of Params: 0.478197\n",
      "Norm of Gradient: 0.052626\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BinaryLogistic-MNIST(init_eta=0.1,batch_size=30,decay_epochs=(10000, 20000),C=1000.0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "model.fit_with_sklearn({'X':X_train, 'y':y_train_fliped})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "909\n",
      "7306 True\n",
      "6426 True\n",
      "1810 True\n",
      "2590 True\n",
      "3088 True\n",
      "1085 True\n",
      "772 True\n",
      "5172 True\n",
      "8532 True\n",
      "13 True\n",
      "2625 True\n",
      "2243 True\n",
      "1905 True\n",
      "8698 True\n",
      "6694 True\n",
      "9074 True\n",
      "7918 True\n",
      "2649 True\n",
      "6533 True\n",
      "8139 True\n",
      "8168 True\n",
      "638 True\n",
      "6846 True\n",
      "4382 True\n",
      "4167 True\n",
      "5149 True\n",
      "7991 True\n",
      "2304 True\n",
      "1962 True\n",
      "6058 True\n",
      "7808 True\n",
      "1330 True\n",
      "2733 True\n",
      "4951 True\n",
      "2410 True\n",
      "3469 True\n",
      "5798 True\n",
      "7965 True\n",
      "2028 True\n",
      "8264 True\n",
      "8004 True\n",
      "8218 True\n",
      "5888 True\n",
      "7135 True\n",
      "3454 True\n",
      "6207 True\n",
      "5330 True\n",
      "6560 True\n",
      "1424 True\n",
      "8154 True\n",
      "2045 True\n",
      "1670 True\n",
      "1920 True\n",
      "4947 True\n",
      "532 True\n",
      "5717 True\n",
      "944 True\n",
      "96 True\n",
      "4562 True\n",
      "7472 True\n",
      "3389 True\n",
      "7407 True\n",
      "1365 True\n",
      "398 True\n",
      "4781 True\n",
      "5484 True\n",
      "5287 True\n",
      "5462 True\n",
      "5540 True\n",
      "6714 True\n",
      "3028 True\n",
      "6763 True\n",
      "6697 True\n",
      "549 True\n",
      "191 True\n",
      "6400 True\n",
      "4381 True\n",
      "3121 True\n",
      "4668 True\n",
      "6678 True\n",
      "8290 True\n",
      "3562 True\n",
      "7471 True\n",
      "8944 True\n",
      "6476 True\n",
      "2002 True\n",
      "1956 True\n",
      "1863 True\n",
      "8144 True\n",
      "8259 True\n",
      "6577 True\n",
      "2972 True\n",
      "425 True\n",
      "1812 True\n",
      "1430 True\n",
      "73 True\n",
      "1089 True\n",
      "3442 True\n",
      "8366 True\n",
      "1311 True\n",
      "4744 True\n",
      "7668 True\n",
      "7483 True\n",
      "4940 True\n",
      "4521 True\n",
      "6169 True\n",
      "1696 True\n",
      "3374 True\n",
      "1463 True\n",
      "7259 True\n",
      "6916 True\n",
      "8757 True\n",
      "1816 True\n",
      "4995 True\n",
      "7745 True\n",
      "2608 True\n",
      "7722 True\n",
      "6160 True\n",
      "8962 True\n",
      "1811 True\n",
      "90 True\n",
      "2428 True\n",
      "420 True\n",
      "2910 True\n",
      "8733 True\n",
      "2073 True\n",
      "3172 True\n",
      "8024 True\n",
      "3959 True\n",
      "5378 True\n",
      "4766 True\n",
      "582 True\n",
      "587 True\n",
      "5438 True\n",
      "574 True\n",
      "175 True\n",
      "1446 True\n",
      "131 True\n",
      "2017 True\n",
      "4933 True\n",
      "4604 True\n",
      "796 True\n",
      "5641 True\n",
      "5340 True\n",
      "5612 True\n",
      "2681 False\n",
      "812 True\n",
      "121 True\n",
      "2636 True\n",
      "7354 True\n",
      "1575 True\n",
      "8035 True\n",
      "6013 True\n",
      "8306 True\n",
      "1259 True\n",
      "2718 True\n",
      "2023 True\n",
      "6970 True\n",
      "5878 True\n",
      "1517 True\n",
      "5670 True\n",
      "7391 True\n",
      "387 True\n",
      "24 True\n",
      "7774 True\n",
      "3464 True\n",
      "7958 True\n",
      "3628 True\n",
      "2170 True\n",
      "5632 True\n",
      "2146 True\n",
      "8429 True\n",
      "6284 True\n",
      "4045 True\n",
      "8960 True\n",
      "606 True\n",
      "1729 True\n",
      "8377 True\n",
      "7915 True\n",
      "5553 True\n",
      "12 True\n",
      "1752 True\n",
      "9069 True\n",
      "7873 True\n",
      "2018 True\n",
      "6840 True\n",
      "4482 True\n",
      "7093 True\n",
      "1354 True\n",
      "3996 True\n",
      "8400 True\n",
      "6060 True\n",
      "2242 True\n",
      "6660 True\n",
      "8442 True\n",
      "1826 True\n",
      "4916 True\n",
      "6298 True\n",
      "5317 True\n",
      "8175 True\n",
      "710\n"
     ]
    }
   ],
   "source": [
    "# bench mark\n",
    "to_be_fixed = set(flip_indices)\n",
    "high_influc =  np.argsort(-np.abs(\n",
    "    model.get_eval(items=['losses'])).T)[0][0:200]\n",
    "#need_to_fix = []\n",
    "y_fixing = np.copy(y_train_fliped)\n",
    "print(len(to_be_fixed))\n",
    "for i in high_influc:\n",
    "    print(i,i in to_be_fixed)\n",
    "    if i in to_be_fixed:\n",
    "        y_fixing[i,:] = 1-y_fixing[i,:]\n",
    "        to_be_fixed -= set([i])\n",
    "        # fix ba\n",
    "print(len(to_be_fixed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Evaluations:\n",
      "------------------------------------------\n",
      "Empirical Risk: 0.370814\n",
      "Norm of Params: 0.519596\n",
      "Norm of Gradient: 0.260626\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BinaryLogistic-MNIST(init_eta=0.1,batch_size=30,decay_epochs=(10000, 20000),C=1000.0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_with_sklearn({'X':X_train, 'y':y_fixing})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "710\n",
      "772 False\n",
      "7306 False\n",
      "2590 False\n",
      "6426 False\n",
      "3088 False\n",
      "1085 False\n",
      "8532 False\n",
      "1810 False\n",
      "5172 False\n",
      "2243 False\n",
      "2625 False\n",
      "2410 False\n",
      "13 False\n",
      "6694 False\n",
      "7965 False\n",
      "1962 False\n",
      "2304 False\n",
      "8698 False\n",
      "3469 False\n",
      "7918 False\n",
      "6533 False\n",
      "2649 False\n",
      "9074 False\n",
      "1330 False\n",
      "638 False\n",
      "5540 False\n",
      "1905 False\n",
      "2733 False\n",
      "2045 False\n",
      "7808 False\n",
      "8259 False\n",
      "532 False\n",
      "5484 False\n",
      "7483 False\n",
      "4167 False\n",
      "6846 False\n",
      "5717 False\n",
      "1365 False\n",
      "8168 False\n",
      "5149 False\n",
      "7991 False\n",
      "3454 False\n",
      "7407 False\n",
      "1863 False\n",
      "8366 False\n",
      "944 False\n",
      "3028 False\n",
      "8139 False\n",
      "8218 False\n",
      "4562 False\n",
      "6697 False\n",
      "5888 False\n",
      "4951 False\n",
      "6560 False\n",
      "5330 False\n",
      "6400 False\n",
      "8290 False\n",
      "1920 False\n",
      "1670 False\n",
      "5462 False\n",
      "5798 False\n",
      "4382 False\n",
      "549 False\n",
      "6169 False\n",
      "8264 False\n",
      "6207 False\n",
      "4744 False\n",
      "8004 False\n",
      "7472 False\n",
      "6160 False\n",
      "1956 False\n",
      "96 False\n",
      "5287 False\n",
      "4381 False\n",
      "6058 False\n",
      "2910 False\n",
      "8962 False\n",
      "6476 False\n",
      "3442 False\n",
      "4995 False\n",
      "1812 False\n",
      "6714 False\n",
      "4947 False\n",
      "2017 False\n",
      "6763 False\n",
      "2608 False\n",
      "4668 False\n",
      "582 False\n",
      "3121 False\n",
      "2002 False\n",
      "7135 False\n",
      "2028 False\n",
      "4781 False\n",
      "6916 False\n",
      "398 False\n",
      "7722 False\n",
      "1811 False\n",
      "3389 False\n",
      "6013 False\n",
      "6284 False\n",
      "90 False\n",
      "4521 False\n",
      "8024 False\n",
      "191 False\n",
      "1424 False\n",
      "1311 False\n",
      "73 False\n",
      "131 False\n",
      "3172 False\n",
      "4604 False\n",
      "7774 False\n",
      "1816 False\n",
      "1517 False\n",
      "7471 False\n",
      "24 False\n",
      "2972 False\n",
      "1430 False\n",
      "7259 False\n",
      "8757 False\n",
      "8154 False\n",
      "7391 False\n",
      "4045 False\n",
      "175 False\n",
      "8960 False\n",
      "8144 False\n",
      "1463 False\n",
      "2428 False\n",
      "9069 False\n",
      "812 False\n",
      "8944 False\n",
      "8400 False\n",
      "5641 False\n",
      "3562 False\n",
      "1089 False\n",
      "7668 False\n",
      "3374 False\n",
      "7873 False\n",
      "8733 False\n",
      "8035 False\n",
      "6577 False\n",
      "5340 False\n",
      "1826 False\n",
      "6674 True\n",
      "4766 False\n",
      "2073 False\n",
      "999 True\n",
      "4933 False\n",
      "420 False\n",
      "4940 False\n",
      "1696 False\n",
      "6970 False\n",
      "4204 True\n",
      "2718 False\n",
      "3893 True\n",
      "574 False\n",
      "193 True\n",
      "5878 False\n",
      "7915 False\n",
      "7093 False\n",
      "3959 False\n",
      "5612 False\n",
      "2023 False\n",
      "4482 False\n",
      "2935 True\n",
      "5378 False\n",
      "7958 False\n",
      "2636 False\n",
      "425 False\n",
      "606 False\n",
      "4916 False\n",
      "342 True\n",
      "2146 False\n",
      "7729 True\n",
      "5632 False\n",
      "587 False\n",
      "7750 True\n",
      "8306 False\n",
      "387 False\n",
      "588 True\n",
      "3822 True\n",
      "2873 True\n",
      "6678 False\n",
      "6397 True\n",
      "1752 False\n",
      "2223 True\n",
      "3464 False\n",
      "6840 False\n",
      "1446 False\n",
      "3259 True\n",
      "5794 True\n",
      "5438 False\n",
      "5930 True\n",
      "8377 False\n",
      "2170 False\n",
      "8429 False\n",
      "2861 True\n",
      "3996 False\n",
      "5553 False\n",
      "5205 True\n",
      "2242 False\n"
     ]
    }
   ],
   "source": [
    "# bench mark\n",
    "# = set(flip_indices)\n",
    "high_influc =  np.argsort(-np.abs(\n",
    "    model.get_eval(items=['losses'])).T)[0][0:200]\n",
    "#need_to_fix = []\n",
    "#y_fixing = np.copy(y_train_fliped)\n",
    "print(len(to_be_fixed))\n",
    "for i in high_influc:\n",
    "    print(i, i in to_be_fixed)\n",
    "    #y_fixing[i,:] = 1-y_fixing[i,:]\n",
    "        # fix ba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.8948701012769705\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_train)\n",
    "print(\"Train accuracy:\", np.sum(y_pred == y_train_fliped)/n_tr)\n",
    "\n",
    "#y_pred_test = model.predict(X_test)\n",
    "#print(\"Test accuracy:\", np.sum(y_pred_test == y_test)/n_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetch training loss gradients (4.449 sec)\n",
      "Influence evaluated for testing point 0.\n",
      "CPU times: user 14.9 s, sys: 1.03 s, total: 16 s\n",
      "Wall time: 7.24 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "I_loss_bf = model.influence_loss(\n",
    "    X_test, y_test,\n",
    "    method='brute-force',\n",
    "    damping=0.0,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.11573096],\n",
       "       [0.06261051],\n",
       "       [0.05120817],\n",
       "       ...,\n",
       "       [0.08391853],\n",
       "       [0.10438586],\n",
       "       [0.09479409]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Evaluations:\n",
      "------------------------------------------\n",
      "Empirical Risk: 0.360206\n",
      "Norm of Params: 0.478197\n",
      "Norm of Gradient: 0.052626\n",
      "Fetch training loss gradients (4.724 sec)\n",
      "Influence evaluated for testing point 0.\n",
      "\n",
      "Model Evaluations:\n",
      "------------------------------------------\n",
      "Empirical Risk: 0.317308\n",
      "Norm of Params: 0.554244\n",
      "Norm of Gradient: 0.061027\n",
      "Fetch training loss gradients (4.913 sec)\n",
      "Influence evaluated for testing point 0.\n",
      "\n",
      "Model Evaluations:\n",
      "------------------------------------------\n",
      "Empirical Risk: 0.269946\n",
      "Norm of Params: 0.605961\n",
      "Norm of Gradient: 0.066702\n",
      "Fetch training loss gradients (4.635 sec)\n",
      "Influence evaluated for testing point 0.\n",
      "\n",
      "Model Evaluations:\n",
      "------------------------------------------\n",
      "Empirical Risk: 0.224690\n",
      "Norm of Params: 0.640409\n",
      "Norm of Gradient: 0.070497\n",
      "Fetch training loss gradients (4.540 sec)\n",
      "Influence evaluated for testing point 0.\n",
      "\n",
      "Model Evaluations:\n",
      "------------------------------------------\n",
      "Empirical Risk: 0.187062\n",
      "Norm of Params: 0.668212\n",
      "Norm of Gradient: 0.073559\n",
      "Fetch training loss gradients (4.472 sec)\n",
      "Influence evaluated for testing point 0.\n"
     ]
    }
   ],
   "source": [
    "to_be_fixed = set(flip_indices)\n",
    "frac_fixed = 0\n",
    "frac_checked = 0\n",
    "y_fixing = np.copy(y_train_fliped)\n",
    "xx = np.zeros(6)\n",
    "yy = np.zeros(6)\n",
    "check_size = 200\n",
    "for batch in range(5):\n",
    "    tf.reset_default_graph()\n",
    "    model.fit_with_sklearn({'X':X_train, 'y':y_fixing })\n",
    "    \n",
    "    I_loss_bf = model.influence_loss(\n",
    "        X_test, y_test,\n",
    "        method='brute-force',\n",
    "        damping=0.0,\n",
    "    )\n",
    "\n",
    "    high_influc = np.argsort(-np.abs(I_loss_bf).T)[0][0:check_size]\n",
    "    need_to_fix = []\n",
    "    \n",
    "    for i in high_influc:\n",
    "        if i in to_be_fixed:\n",
    "            # fix back\n",
    "            y_fixing[i,:] = 1-y_fixing[i,:]\n",
    "            frac_fixed += (1/909)\n",
    "            need_to_fix.append(i)\n",
    "            \n",
    "    to_be_fixed = to_be_fixed - set(need_to_fix)\n",
    "            \n",
    "    frac_checked += (check_size/n_tr)\n",
    "    xx[batch+1] = frac_checked\n",
    "    yy[batch+1] = frac_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zed/anaconda/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "\n",
    "from data import Datafile, load_data\n",
    "from influence.emp_risk_optimizer import EmpiricalRiskOptimizer\n",
    "from influence.plot_utils import compare_with_loo\n",
    "from influence.closed_forms import I_loss_RidgeCf\n",
    "from models.regularized_regression import RegularizedRegression\n",
    "from models.hyperplane_clf import BinaryLogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (9084, 784)\n",
      "y_train shape: (9084, 1)\n",
      "X_test shape: (1, 784)\n",
      "y_test shape: (1, 1)\n",
      "9084 784\n",
      "[7636  371 2974 7343 6777 3893 7962 4771 5238  285 6331 3774  641 8425\n",
      " 1073 3687 6182 8680 7294 8091 2137 3886 4879 6860 1135 6004 8168    9\n",
      " 2296  574 2262 7093 2649 8473 5683 7493 1913 6160 3702 2002 9014 4300\n",
      " 5675 3870  143 7808 2873 2652 4387 9019 2625 5454 4635 1745 6087 3678\n",
      "  874 7915 2836 6405 4499 2457 3888 5388  532 5484 5146 6970 7785 3174\n",
      " 5949  156 5089 2803 1541  402 8429 2789 3320 5453 6763 1533 1614 2442\n",
      " 8377 4113 5569 4973 1075 4794 1848 5985 6840 3295 6342 3815 6694  781\n",
      " 2331 4912 4940 4916  281 7943 1905 3054  420  822 4301   80  713 2292\n",
      " 7508 5817 8474 4965  842 5147 8603 1899 1239 6125 3507 2861 8959  588\n",
      " 6855 7025 4370 4562 5192 8313 6177 8811 3364 8948 6914 4951 2315   90\n",
      " 3617 3828 3027 8480 2431 8766 7354  315 2710 1463 6577 6476 3743 8478\n",
      " 8832 3850 1429 6734 5918 2290 2475 2787 3454 7214 9011   12 2864 5661\n",
      " 9069 4485 1752 4292 1354 6243 6254 1811 7467 2051 1337  263  705 2608\n",
      " 5065 4584 3183 1651 1233 3959  317  819 7869 2809 6060 6958 3823 5232\n",
      " 6607 3462 1542 7668  723 7051 5835  610 7357 3348 7078 1384 7828 8954\n",
      " 3464 4604 5501 2975 8485 2017 2986 7306 6533 3182 1314 6207 6560 8024\n",
      " 1916 2022  488 4649 4837 2309 8154 3777 2191 7732 5344 4668 4766 5179\n",
      " 2827 6846 3748  601 7122 4990 7893 3934 6058 8899 7458 5172 5751 8921\n",
      "  131 6660 8214 1179 4619 1550 2023 4437 5006 1376 5498 1800 8815 1863\n",
      " 5958 4021 8706 7342 1878 6678 1579 1378 8400 8672 1430 8657 5196 2675\n",
      " 4496  502 1672 7312 7095 7958 6689   96 7864 1258 6169 5154  529  569\n",
      " 5632 6319 2930  369 5761 3389 2422 6783 7754  431  835 3628 8627 3518\n",
      " 8157 8379 8165 4410 1275 4644 6961 2042 1805 5515  455 8532  688 6009\n",
      " 6325 6598 2935 1165 2896 3258 7096 2919 8350 5064  638 7923  707 5016\n",
      " 6697 1566 4888  337 7072 6426 5679 1962 2170 4936  398 4961 8144 4258\n",
      " 8465 1812 5878 6029  474 1096 8373 4746 2622 3121 5062 2305 3343  788\n",
      " 5149 5722 8868 6714 5483 1920 4947 7908  657 7965   24 8757 4087 2451\n",
      " 1308 3002 7736 8264 1826 8269   45 5816 4122 1756 1606 6267 1673 5930\n",
      "  193 8962 5988 1517 5462 1816 7374 8810 3144 5056 5733  397 3822 1819\n",
      " 8626  385 5540 1978 5602 5940 3433 2046 3653 2223 1729 8170 8495  963\n",
      " 3055 3151 4587 3028 6333 3711 7460 1448 4097 2335 6050 8841 7259 1790\n",
      " 2356 6772  191 7676  999 3000 3738 2018 5508 5570 6284 1955 1445 7391\n",
      " 4995 8198 3221 2010 3407 1401  772 1513  443 8413 2351 4045 4010 3835\n",
      " 4781 7991 7774 2534 5612 5318  796 2428 7729 2153 3469 1259 2779 1431\n",
      "    4 6588 3259 7873 4871 1089  342 1397 2242 8321 2940 2055 6504 3723\n",
      " 8533 7480 1309 3689 2497 3976 8585  125 2910 3381 1366 2396 4731  694\n",
      " 8442 8944 3393 6913 6920 2598 8187 1141 6559 5879 3560 7605 8960 5986\n",
      " 8237 7877 5798 8472 4782 3431  534 8089 6219 8226 2266 4381 3273 1992\n",
      "  636 4008 4839 2410 2653 7903  861 5537 4317 1095 7693  312 1887 5491\n",
      " 2237  466 4185  741 6803 1876 7950 3085 1633 5330 7455 7345 4167 4131\n",
      " 6397 2304 7242 1361 5900 6869 5998 5695 5003  190  944 8705 6020 2336\n",
      " 8101 5788 2514 6916 2243 7623 1670 6400 1595 3360 2045 2718 2146 7745\n",
      "  257 4642 4752 2588 7295 6674 7918 2420 2142  121  218 3065 3374 1804\n",
      " 2392 8915 4038 4019 5906 4842 7932  587 8881  293 6496 4466 6627 1085\n",
      " 3058 7848 5438  595 4458 4522  305 4382 3501 5782 8194 4407  812 7039\n",
      " 1330 1561 5219 6184 3996 7666 8306 6163 3298  120 1322  770 4681 7472\n",
      " 3435 3108 2972 1972 2073 5287 1671 9038 1311 2250 4612 4014 1678 5929\n",
      "  926 4204 8249 1103 7944 3126 7483 7407 3414 7671 1387 2065 3442 2049\n",
      " 1234 1775 7344 8175 7351 4026 8366 1538  411 7787  463 7750 2141 3894\n",
      " 6013 4827 5378 7011 8546 6925 4247 5794 4677 4071 2720 4874 1062 8290\n",
      " 8108 4022 6347 1189 5204 4963 7947 8129 6063 1634 2978 7852 4482   73\n",
      "  658 2645 3994 8670 1539 8725 8218 4545 4705 4823  697 7303 3669 8021\n",
      " 4749 4117 5931 4198 4688 5108 7178 3248 3355 8517  311 4744 8496 3875\n",
      " 4890 3395 8103 4471 8032 5820  558  582 2590 3818 5888 6056 3706 2183\n",
      " 5162 1365 7086 4933 8320  233 8035 3562 6582 8004 3937 8967 1424 5717\n",
      " 1694 7558 1801 8016 6953 6418 6298 2826 7619 2140 8698 2733 7334  387\n",
      " 7665 4822 3665 2026 3409 5749 1368 3172 4266 2983 9074 5360 5641  827\n",
      " 8369 6465 1696 2284 3602 3088 4694 3634 4650 7722 4177 4068 5317 1040\n",
      "  994 6808 8169 8139 9006 4624 2227 8808 7192 5340 8136 1814 2459 6959\n",
      "   51  783 7143 2349 3211 1556 4673 6224 1350 7272 8733 7135 7471  350\n",
      " 3415   15 5205  425 8341 2156 1810 5057 5107 5553 1575 8212  175 6221\n",
      " 1665 1168 2028  549 8204  313 7382 1356 1083  606 5738 2636 1130 8345\n",
      "  651 4753 7046 2423 1446 1928 7989 3041 7936 4521 7792 7934 3095 1956\n",
      " 4819 4478 5645 5073   13 8415  358 5909 7102 8946 2328 5830 1722  349\n",
      " 6330 3181 3528 1691   10 2558 1193 5670 8221  613 8816 8259 5435]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zed/anaconda/lib/python3.6/site-packages/sklearn/utils/validation.py:429: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "# flip 10% indices\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X_train, X_test, y_train, y_test, test_indices = load_data(\n",
    "    Datafile.BinaryMNIST17, test_config=[0])\n",
    "n_tr, p = X_train.shape\n",
    "n_te, _ = X_test.shape\n",
    "print(n_tr, p)\n",
    "\n",
    "\n",
    "scl_x = StandardScaler()\n",
    "scl_y = StandardScaler()\n",
    "X_train = scl_x.fit_transform(X_train)\n",
    "#X_test = scl_x.transform(X_test)\n",
    "\n",
    "np.random.seed(43)\n",
    "flip_indices = np.random.choice(n_tr, size=909, replace=False)\n",
    "y_train_fliped = np.copy(y_train)\n",
    "y_train_fliped[flip_indices,:] = 1-y_train_fliped[flip_indices,:]\n",
    "\n",
    "print(flip_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BinaryLogisticRegression(\n",
    "    model_name='BinaryLogistic-MNIST',\n",
    "    init_eta=0.01,\n",
    "    decay_epochs=[200, 400, 800],\n",
    "    batch_size=100,\n",
    "    C=1e3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8999339498018494\n",
      "909\n",
      "\n",
      "Model Evaluations:\n",
      "------------------------------------------\n",
      "Empirical Risk: 0.360206\n",
      "Norm of Params: 0.478197\n",
      "Norm of Gradient: 0.052626\n",
      "710\n",
      "0.9218405988551299\n",
      "\n",
      "Model Evaluations:\n",
      "------------------------------------------\n",
      "Empirical Risk: 0.296337\n",
      "Norm of Params: 0.519596\n",
      "Norm of Gradient: 0.057194\n",
      "511\n",
      "0.9437472479084104\n",
      "\n",
      "Model Evaluations:\n",
      "------------------------------------------\n",
      "Empirical Risk: 0.235338\n",
      "Norm of Params: 0.573272\n",
      "Norm of Gradient: 0.063111\n",
      "312\n",
      "0.9656538969616909\n",
      "\n",
      "Model Evaluations:\n",
      "------------------------------------------\n",
      "Empirical Risk: 0.173446\n",
      "Norm of Params: 0.633636\n",
      "Norm of Gradient: 0.069750\n",
      "118\n",
      "0.9870101276970498\n",
      "\n",
      "Model Evaluations:\n",
      "------------------------------------------\n",
      "Empirical Risk: 0.118093\n",
      "Norm of Params: 0.711241\n",
      "Norm of Gradient: 0.078295\n",
      "13\n",
      "0.9985689123734038\n"
     ]
    }
   ],
   "source": [
    "to_be_fixed = set(flip_indices)\n",
    "frac_fixed = 0\n",
    "frac_checked = 0\n",
    "y_fixing = np.copy(y_train_fliped)\n",
    "print(np.mean(y_fixing == y_train))\n",
    "print(len(to_be_fixed))\n",
    "xx = np.zeros(6)\n",
    "zz = np.zeros(6)\n",
    "check_size = 200\n",
    "for batch in range(5):\n",
    "    tf.reset_default_graph()\n",
    "    model.fit_with_sklearn({'X':X_train, 'y':y_fixing })\n",
    "    \n",
    "    \"\"\"\n",
    "    I_loss_bf = model.influence_loss(\n",
    "        X_test, y_test,\n",
    "        method='brute-force',\n",
    "        damping=0.0,\n",
    "    )\n",
    "    \"\"\"\n",
    "    high_influc =  np.argsort(-np.abs(\n",
    "        model.get_eval(items=['losses'])).T)[0][0:check_size]\n",
    "    need_to_fix = []\n",
    "    \n",
    "    for i in high_influc:\n",
    "        if i in to_be_fixed:\n",
    "            # fix back\n",
    "            y_fixing[i,:] = 1-y_fixing[i,:]\n",
    "            frac_fixed += (1/909)\n",
    "            need_to_fix.append(i)\n",
    "            \n",
    "    to_be_fixed = to_be_fixed - set(need_to_fix)\n",
    "    print(len(to_be_fixed))\n",
    "    print(np.mean(y_fixing == y_train))\n",
    "    frac_checked += (check_size/n_tr)\n",
    "    xx[batch+1] = frac_checked\n",
    "    zz[batch+1] = frac_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.21892189 0.43784378 0.65676568 0.87018702 0.98569857]\n"
     ]
    }
   ],
   "source": [
    "print(zz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x13958c6d8>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAecAAAFJCAYAAAChG+XKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nbts1wmAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x123806240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "frac_data_check = np.copy(xx)\n",
    "frac_fixed_influence = np.copy(yy)\n",
    "plt.plot(frac_data_check,frac_fixed_influence)\n",
    "plt.plot(frac_data_check,zz)\n",
    "plt.plot(frac_data_check,frac_data_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.21892189, 0.23982398, 0.24092409, 0.2420242 ,\n",
       "       0.24312431])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
